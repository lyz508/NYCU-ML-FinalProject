{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "c8f3ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import display_markdown\n",
    "import joblib\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "c174e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configurations\n",
    "class Configs:\n",
    "    def __init__(self, debug=False) -> None:\n",
    "        self.path_prefix = \"./tabular-playground-series-aug-2022\"\n",
    "        self.train_path = \"./tabular-playground-series-aug-2022/train.csv\"\n",
    "        self.test_path = \"./tabular-playground-series-aug-2022/test.csv\"\n",
    "        self.store_path = \"./model\"\n",
    "        self.submission_path = \"./tabular-playground-series-aug-2022/sample_submission.csv\"\n",
    "        self.is_debug = debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "f39ee227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for data preprocessing\n",
    "\"\"\"\n",
    "def corr_selection(data, test):\n",
    "    # Select columns with correlation and apply imputation\n",
    "    # Means to select best correlation columns according to the product code\n",
    "    full_dict = {}\n",
    "    num_get = 7 # get top 7 columns\n",
    "    # Append additional attributes\n",
    "    col = [col for col in test.columns if 'measurement' not in col] \\\n",
    "        + ['loading', 'm3_missing', 'm5_missing', 'count_null', 'area']\n",
    "    correlations = []\n",
    "    selected =[]\n",
    "\n",
    "    # imputation for measurement3 ~ 17 (start from missing)\n",
    "    for m_num in range(3,18):\n",
    "        correlation = data.drop(col, axis=1).corr()[f'measurement_{m_num}']\n",
    "        correlation = np.absolute(correlation).sort_values(ascending=False)\n",
    "        # selection with correlation sum\n",
    "        correlations.append(np.round(np.sum(correlation[1:5]), 5))\n",
    "        selected.append(f'measurement_{m_num}')\n",
    "        \n",
    "    # select columns\n",
    "    selected_col = pd.DataFrame()\n",
    "    selected_col['selected'] = selected\n",
    "    selected_col['correlation'] = correlations\n",
    "    selected_col = selected_col.sort_values(by='correlation',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # select columns to apply imputation\n",
    "    # much unused, only use the first 3 imputation\n",
    "    # some attrs (like count null) may be affected by imputation, do imputation for those datas\n",
    "    for i in range(num_get):\n",
    "        target_cols = 'measurement_' + selected_col.iloc[i,0][12:] # selection for next best\n",
    "        fill_dict ={}\n",
    "        for x in data.product_code.unique() : \n",
    "            # Compute correlation for all product code\n",
    "            correlation = data[data.product_code == x].drop(col, axis=1).corr()[target_cols]\n",
    "            correlation = np.absolute(correlation).sort_values(ascending=False)\n",
    "            target_cols_dic = {}\n",
    "            target_cols_dic[target_cols] = correlation[1:5].index.tolist()\n",
    "            fill_dict[x] = target_cols_dic[target_cols]\n",
    "        full_dict[target_cols] = fill_dict\n",
    "\n",
    "    # display selected columns\n",
    "    display_markdown('## Selected columns by the sum of correlation', raw=True)\n",
    "    display(selected_col.head(10))\n",
    "    \n",
    "    return full_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "25a4b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Program initialization\n",
    "1. filtering warnings\n",
    "2. create global configuration\n",
    "3. read data indicated by configuration\n",
    "4. describe the data if 'debug' mode is opened in configs\n",
    "\"\"\"\n",
    "warnings.filterwarnings('ignore')            # Ignore warnings with filter\n",
    "configs = Configs(False)                     # Global Configuration for easy accessing\n",
    "\n",
    "# Read Data\n",
    "train = pd.read_csv(f\"{configs.train_path}\")\n",
    "test = pd.read_csv(f\"{configs.test_path}\")\n",
    "submission = pd.read_csv(f\"{configs.submission_path}\")\n",
    "\n",
    "# Dataset description\n",
    "if configs.is_debug:\n",
    "    print(f\"Data Shape: [\\n\\\n",
    "        train: {train.shape}, \\n\\\n",
    "        test: {test.shape}\\n\\\n",
    "    ]\")\n",
    "    print(f\"Label in train dataset: [ \\n\\\n",
    "        failure: {train[train.failure==0].shape[0]}, \\n\\\n",
    "        success: {train[train.failure==1].shape[0]}\\n\\\n",
    "    ]\") # get corresponding row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "67538026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data-Preprocessing\n",
    "1. create or aggregate features to get new features\n",
    "2. make imputation for missing values with their most correlation attributes\n",
    "\"\"\"\n",
    "def data_preprocessing(train, test):\n",
    "    # Aggregate orb create new values by discussions mentioned in report\n",
    "    display_markdown('# Imputation', raw=True)\n",
    "    data = pd.concat([train, test])\n",
    "    data['area'] = data['attribute_2'] * data['attribute_3']\n",
    "    data['count_null'] = data.isnull().sum(axis=1)\n",
    "    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n",
    "    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n",
    "    data['loading'] = np.log1p(data['loading'])\n",
    "\n",
    "    # locate for all measurement\n",
    "    features = [f for f in test.columns if f.startswith('measurement') or f=='loading']\n",
    "\n",
    "    # Two filling method\n",
    "    # 1. fill with linear model (LinearRegression)\n",
    "    # 2. fill with KNN model\n",
    "    full_dict = corr_selection(data, test)\n",
    "    display_markdown('## Imputation with Linear Model and KNN Model', raw=True)\n",
    "    for code in data.product_code.unique():\n",
    "        # Fill with LinearRegression\n",
    "        for m_col in list(full_dict.keys()):\n",
    "            corress_data = data[data.product_code==code]\n",
    "            column = full_dict[m_col][code]\n",
    "            tmp_train = corress_data[column + [m_col]].dropna(how='any')\n",
    "            tmp_test = corress_data[\n",
    "                (corress_data[column].isnull().sum(axis=1)==0) & (corress_data[m_col].isnull())\n",
    "            ]\n",
    "            huber_regressor = HuberRegressor(epsilon=1.35, max_iter = 1000)\n",
    "            huber_regressor.fit(tmp_train[column], tmp_train[m_col])\n",
    "            joblib.dump(huber_regressor, f\"{configs.store_path}/Huber_{code}_{m_col}\") # dump for inference\n",
    "            data.loc[(data.product_code==code)                  # Imputation on selected missing fields\n",
    "                     & (data[column].isnull().sum(axis=1)==0)\n",
    "                     & (data[m_col].isnull()), m_col] = huber_regressor.predict(tmp_test[column])\n",
    "            if configs.is_debug:\n",
    "                print(f\"{m_col}: code {code}, {len(tmp_test)} samples\")\n",
    "        print(f\"code {code} have been imputed by LinearRegressor\")\n",
    "        # Fill with KNN\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        joblib.dump(imputer, f\"{configs.store_path}/imputer_{code}\")\n",
    "        data.loc[data.product_code==code, features] = imputer.fit_transform(data.loc[data.product_code==code, features])\n",
    "        print(f\"code {code} have been imputed by KNN\")\n",
    "    print(\"Done Inputation\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "c2d9eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Imputation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Selected columns by the sum of correlation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>measurement_17</td>\n",
       "      <td>1.43140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>measurement_8</td>\n",
       "      <td>0.46499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>measurement_11</td>\n",
       "      <td>0.41957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>measurement_5</td>\n",
       "      <td>0.39309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>measurement_6</td>\n",
       "      <td>0.37335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>measurement_7</td>\n",
       "      <td>0.34639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>measurement_4</td>\n",
       "      <td>0.33899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>measurement_16</td>\n",
       "      <td>0.30221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>measurement_10</td>\n",
       "      <td>0.28251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>measurement_14</td>\n",
       "      <td>0.25259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         selected  correlation\n",
       "0  measurement_17      1.43140\n",
       "1   measurement_8      0.46499\n",
       "2  measurement_11      0.41957\n",
       "3   measurement_5      0.39309\n",
       "4   measurement_6      0.37335\n",
       "5   measurement_7      0.34639\n",
       "6   measurement_4      0.33899\n",
       "7  measurement_16      0.30221\n",
       "8  measurement_10      0.28251\n",
       "9  measurement_14      0.25259"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Imputation with Linear Model and KNN Model"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code A have been imputed by LinearRegressor\n",
      "code A have been imputed by KNN\n",
      "code B have been imputed by LinearRegressor\n",
      "code B have been imputed by KNN\n",
      "code C have been imputed by LinearRegressor\n",
      "code C have been imputed by KNN\n",
      "code D have been imputed by LinearRegressor\n",
      "code D have been imputed by KNN\n",
      "code E have been imputed by LinearRegressor\n",
      "code E have been imputed by KNN\n",
      "code F have been imputed by LinearRegressor\n",
      "code F have been imputed by KNN\n",
      "code G have been imputed by LinearRegressor\n",
      "code G have been imputed by KNN\n",
      "code H have been imputed by LinearRegressor\n",
      "code H have been imputed by KNN\n",
      "code I have been imputed by LinearRegressor\n",
      "code I have been imputed by KNN\n",
      "Done Inputation\n"
     ]
    }
   ],
   "source": [
    "# Use function\n",
    "data = data_preprocessing(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "b2576030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear Regression Model\n",
    "0. Use KFold for input training data\n",
    "1. train multiple linear regressor preventing overfitting\n",
    "2. evaluation the linear regressor by auc and acc\n",
    "3. return results of the test data\n",
    "\"\"\"\n",
    "class LRModel:\n",
    "    def __init__(self, data, features):\n",
    "        self.x = data.drop(['failure'], axis=1)\n",
    "        self.y = data['failure'].astype(int)\n",
    "        self.features = features\n",
    "        self.n_splits = 5 # KFold Spilt\n",
    "        self.data_len = len(data)\n",
    "        self.lr_auc, self.lr_preds = 0, 0\n",
    "        # evaluation of model\n",
    "        self.x_trains = []\n",
    "        self.train_aucs = []\n",
    "        self.train_accs = []\n",
    "        self.oof_aucs = []\n",
    "        self.oof_accs = []\n",
    "        self.feats_evaluations = []\n",
    "        self.lr_effects = [0.3, 0.3, 0.2, 0.2]\n",
    "        self.lr_results = []        # prediction results\n",
    "        # construct feats subset to train multiple LR\n",
    "        self.subsets = []\n",
    "        self._construct_subset()\n",
    "        \n",
    "    def _construct_subset(self): \n",
    "        # fixed chosen subset for better results\n",
    "        self.subsets.append(self.features)\n",
    "        tmp_list = [0, 1, 5, 6, 7]\n",
    "        self.subsets.append([self.features[x] for x in tmp_list])\n",
    "        tmp_list = [0, 2, 3, 4, 6, 7]\n",
    "        self.subsets.append([self.features[x] for x in tmp_list])\n",
    "        tmp_list = [0, 5, 7]\n",
    "        self.subsets.append([self.features[x] for x in tmp_list])\n",
    "        \n",
    "    def _get_accuracy_n(self, valid, preds, n_splits):\n",
    "        return round(accuracy_score(valid, preds), n_splits)\n",
    "    \n",
    "    def _get_auc_n(self, valid, preds, n_splits):\n",
    "        return round(roc_auc_score(valid, preds), n_splits)\n",
    "\n",
    "    def _scale(self, train_data, val_data, test_data, feats):\n",
    "        # function for cross validation, scaling the data of x_train, x_val, x_test by selected feature\n",
    "        scaler = StandardScaler()                                 # Gaussion distribussion\n",
    "        # transform datas with specific feats\n",
    "        scaled_train = scaler.fit_transform(train_data[feats])  \n",
    "        scaled_val = scaler.transform(val_data[feats])\n",
    "        scaled_test = scaler.transform(test_data[feats])\n",
    "        # make return value\n",
    "        rtn_train = train_data.copy()\n",
    "        rtn_val = val_data.copy()\n",
    "        rtn_test = test_data.copy()\n",
    "        # replace specific feats with standardize datas\n",
    "        rtn_train[feats] = scaled_train\n",
    "        rtn_val[feats] = scaled_val\n",
    "        rtn_test[feats] = scaled_test\n",
    "        return rtn_train, rtn_val, rtn_test\n",
    "    \n",
    "    def train(self, test_data):\n",
    "        # train multiple LRs\n",
    "        for i_feat, feats in enumerate(self.subsets):\n",
    "            lr_auc, lr_preds = 0, 0\n",
    "            lr_target = np.zeros(len(test_data))\n",
    "            lr_oof_auc, lr_oof_preds = np.zeros(self.data_len), np.zeros(self.data_len)\n",
    "            feats_evaluation = []\n",
    "    \n",
    "            # KFold\n",
    "            kfold = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=0)\n",
    "            for _, (train_idx, valid_idx) in enumerate(kfold.split(self.x, self.y)):\n",
    "                x_train, x_val = self.x.iloc[train_idx], self.x.iloc[valid_idx]\n",
    "                y_train, y_val = self.y.iloc[train_idx], self.y.iloc[valid_idx]\n",
    "                x_test = test_data.copy()\n",
    "                x_train, x_val, x_test = self._scale(x_train, x_val, x_test, feats)\n",
    "                # train linear regressor\n",
    "                lr = LogisticRegression(max_iter=1000, C=0.0001, penalty='l2', solver='newton-cg')\n",
    "                lr.fit(x_train[feats], y_train)\n",
    "                joblib.dump(lr, f\"{configs.store_path}/LRModel_{i_feat}_{_}\")  # dump model\n",
    "                val_preds = lr.predict_proba(x_val[feats])[:, 1]\n",
    "                y_preds = lr.predict(x_val[feats])\n",
    "                lr_target += lr.predict_proba(x_test[feats])[:, 1] / self.n_splits\n",
    "                \n",
    "                # Store constants for evaluation\n",
    "                feats_evaluation.append(lr.coef_.ravel())      # evaluate importance of features\n",
    "                lr_auc += roc_auc_score(y_val, val_preds) / self.n_splits\n",
    "                lr_preds += accuracy_score(y_val, y_preds) / self.n_splits\n",
    "                lr_oof_auc[valid_idx] = val_preds\n",
    "                lr_oof_preds[valid_idx] = y_preds\n",
    "            \n",
    "            # Store result and evaluations\n",
    "            self.x_trains.append(x_train)\n",
    "            self.lr_results.append(lr_target)\n",
    "            self.train_aucs.append(round(lr_auc, self.n_splits))\n",
    "            self.train_accs.append(round(lr_preds, self.n_splits))\n",
    "            self.oof_aucs.append(self._get_auc_n(self.y, lr_oof_auc, self.n_splits))\n",
    "            self.oof_accs.append(self._get_accuracy_n(self.y, lr_oof_preds, self.n_splits))\n",
    "            self.feats_evaluations.append(feats_evaluation)\n",
    "            \n",
    "    def evaluation(self):\n",
    "        # print evaluation information\n",
    "        for i, feats in enumerate(self.subsets):\n",
    "            display_markdown(f\"## Model Evaluation for lr{i}\", raw=True)\n",
    "            display_markdown(f\"- Train auc = {self.train_aucs[i]} \\\n",
    "                \\n- Train acc = {self.train_accs[i]}\", raw=True)\n",
    "            display_markdown(f\"- oof auc = {self.oof_aucs[i]} \\\n",
    "                \\n- oof acc = {self.oof_accs[i]}\", raw=True)\n",
    "            \n",
    "            display_markdown(\"### Importance List\", raw=True)\n",
    "            importancement = pd.DataFrame(\n",
    "                np.array(self.feats_evaluations[i]).T,\n",
    "                index=self.x_trains[i][feats].columns\n",
    "            ).mean(axis=1).abs().sort_values(ascending=False)\n",
    "            display(importancement.head())\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "a5fe4009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Model Evaluation for lr0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Train auc = 0.59124                 \n",
       "- Train acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- oof auc = 0.59114                 \n",
       "- oof acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Importance List"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "loading           0.081209\n",
       "measurement_17    0.021065\n",
       "measurement_2     0.010496\n",
       "m5_missing        0.010240\n",
       "m3_missing        0.010068\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Model Evaluation for lr1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Train auc = 0.59074                 \n",
       "- Train acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- oof auc = 0.59065                 \n",
       "- oof acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Importance List"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "loading           0.081218\n",
       "measurement_17    0.021076\n",
       "measurement_2     0.010483\n",
       "area              0.010008\n",
       "measurement_1     0.006987\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Model Evaluation for lr2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Train auc = 0.59066                 \n",
       "- Train acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- oof auc = 0.59059                 \n",
       "- oof acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Importance List"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "loading           0.081226\n",
       "measurement_17    0.021070\n",
       "m5_missing        0.010264\n",
       "measurement_2     0.010225\n",
       "m3_missing        0.010105\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Model Evaluation for lr3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Train auc = 0.59021                 \n",
       "- Train acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- oof auc = 0.59019                 \n",
       "- oof acc = 0.78739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Importance List"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "loading           0.081239\n",
       "measurement_17    0.021045\n",
       "measurement_1     0.006989\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training with defined LRModel\n",
    "1. get train and test dataset ready\n",
    "2. select features (according to some discussions mentioned in report)\n",
    "3. init the training model and predict\n",
    "4. get returned results\n",
    "5. save to .csv file\n",
    "\"\"\"\n",
    "# reassign train and test data since concat before\n",
    "# test -> need prediction the failure, choose failure field is empty\n",
    "train = data[data.failure.notnull()]\n",
    "test = data[data.failure.isnull()].drop(['failure'], axis=1)\n",
    "\n",
    "# select features\n",
    "select_feature = [\n",
    "    'loading', \n",
    "    'area', 'count_null',\n",
    "    'm3_missing', 'm5_missing', \n",
    "    'measurement_1', 'measurement_2', 'measurement_17'\n",
    "]\n",
    "\n",
    "# Init and train model\n",
    "lrmodel = LRModel(train, select_feature)\n",
    "lrmodel.train(test_data=test)\n",
    "lrmodel.evaluation()\n",
    "\n",
    "# get results\n",
    "submission['lr0'], submission['lr1'], submission['lr2'], submission['lr3'] = lrmodel.get_results()\n",
    "submission['failure'] = submission['lr0'] * lrmodel.lr_effects[0] + \\\n",
    "    submission['lr1'] * lrmodel.lr_effects[1] + \\\n",
    "    submission['lr2'] * lrmodel.lr_effects[2] +\\\n",
    "    submission['lr3'] * lrmodel.lr_effects[2]\n",
    "submission.head()\n",
    "\n",
    "# write to csv\n",
    "submission[['id', 'failure']].to_csv('109550129_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064dbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e72b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
